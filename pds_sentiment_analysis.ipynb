{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "pds_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n42TU-oY6a_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unzipping\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"required_files.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i_7A6TfhytD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa5hnNcw2-q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(query, location):\n",
        "    # print()\n",
        "    # print(query, location)\n",
        "    #Get the data, either from cache or from the internet\n",
        "    path = \"required_files/\"\n",
        "    try:\n",
        "        with open(path + query + \"_\" + location + \"_cache.txt\", \"r\") as f:\n",
        "            data = json.loads(f.read())\n",
        "        # print(\"Found cache\")\n",
        "    except:\n",
        "        urls = get_urls(query, location)\n",
        "        print(\"Collecting data from the internet\")\n",
        "        data = get_data_from_internet(urls)\n",
        "        with open(path + query + \"_\" + location + \"_cache.txt\", \"w\") as f:\n",
        "            if (data != []):\n",
        "                f.write(json.dumps(data))\n",
        "    print(f\"Collected {len(data):3d} unique tweets for this query and location\")\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlLbBUmz2-q4",
        "colab_type": "code",
        "outputId": "9f3f6fbd-d784-43dc-edd3-7c77c74451c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "#This url can be intuitively modified to suit our query\n",
        "#This specific url searches for tweets mentioning @realDonaldTrump between the dates 2016-10-08 and 2016-11-08\n",
        "queries = [\"trump\", \"clinton\"]\n",
        "locations = [\"philadelphia\", \"chester\", \"belmont\", \"hamilton\", \"newhanover\", \n",
        "             \"wake\", \"watauga\", \"duval\", \"hillsborough\", \"miamidade\", \n",
        "             \"allegheny\", \"atlantic\", \"maricopa\"]\n",
        "data_dict = dict()\n",
        "for query in queries:\n",
        "    for location in locations:\n",
        "        data_dict[(query, location)] = get_data(query, location)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collected 147 unique tweets for this query and location\n",
            "Collected  56 unique tweets for this query and location\n",
            "Collected   7 unique tweets for this query and location\n",
            "Collected  58 unique tweets for this query and location\n",
            "Collected  15 unique tweets for this query and location\n",
            "Collected  47 unique tweets for this query and location\n",
            "Collected 238 unique tweets for this query and location\n",
            "Collected  72 unique tweets for this query and location\n",
            "Collected  72 unique tweets for this query and location\n",
            "Collected 142 unique tweets for this query and location\n",
            "Collected  76 unique tweets for this query and location\n",
            "Collected  47 unique tweets for this query and location\n",
            "Collected 210 unique tweets for this query and location\n",
            "Collected 111 unique tweets for this query and location\n",
            "Collected  22 unique tweets for this query and location\n",
            "Collected   5 unique tweets for this query and location\n",
            "Collected  44 unique tweets for this query and location\n",
            "Collected   4 unique tweets for this query and location\n",
            "Collected  58 unique tweets for this query and location\n",
            "Collected 220 unique tweets for this query and location\n",
            "Collected  45 unique tweets for this query and location\n",
            "Collected 107 unique tweets for this query and location\n",
            "Collected 169 unique tweets for this query and location\n",
            "Collected  67 unique tweets for this query and location\n",
            "Collected   8 unique tweets for this query and location\n",
            "Collected 148 unique tweets for this query and location\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tuy8lJNW4oMt",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Anaylysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgCefoBX4wXf",
        "colab_type": "text"
      },
      "source": [
        "We will train a sentiment analysis model on stanford's Sentiment140 dataset. It is a dataset of tweets, many of which are about political topics. Upon manual inspection, this dataset looks like training on it will generalize well to our specific data which is also tweets and is solely about politics. Note that this dataset is not the focus of this project and is only being used to train a sentiment analysis model which will be used to analyze our actual dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwsnVcZ94un9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# If you use this data, please cite Sentiment140 as your source.\n",
        "data_path = \"required_files/trainingandtestdata/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "training_data = pd.read_csv(data_path, encoding=\"latin1\", header=None)\n",
        "training_data = training_data.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwrv0gVm802U",
        "colab_type": "code",
        "outputId": "51fc9e98-4cef-4de0-f566-b4b984159e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# parse data into tweets and correponding labels (numpy arrays)\n",
        "import collections\n",
        "import nltk\n",
        "# for package in [\"wordnet\", \"punkt\", \"stopwords\"]:\n",
        "#   nltk.download(package)\n",
        "\n",
        "labels = training_data[0].to_numpy()\n",
        "tweets = training_data[5].to_numpy()\n",
        "\n",
        "#TODO: add some more preprocessing, like punctuation, tokenization etc..\n",
        "for i in range(len(tweets)):\n",
        "  tweets[i] = tweets[i].lower()\n",
        "\n",
        "nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def is_stop_word(word):\n",
        "    if word in nltk_stop_words: #first layer is nltk stopwords\n",
        "      return True\n",
        "    if (word in [\"\", \" \"]): #removing empty strings\n",
        "      return True\n",
        "    if \"http\" in word: #removing urls\n",
        "      return True\n",
        "    #remove topic of analysis for ubiased sentiment estimate\n",
        "    if word in [\"trump\", \"donald\", \"hillary\", \"clinton\"]:\n",
        "      return True\n",
        "    #if not a stopword, use it\n",
        "    return False\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVb6kqbJ_jRM",
        "colab_type": "code",
        "outputId": "6b718fa7-7fe2-424a-c12d-bbd9ff27cce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "#TODO: make tweets into bag of words representation (check past hw)\n",
        "#TODO: alternatively, make into reverse index representation\n",
        "\n",
        "import random\n",
        "import re\n",
        "from nltk.classify import util\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import movie_reviews, stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "#strip_handles makes it remove usernames and reduce_len deletes repeated letters\n",
        "#example: huuuuuuuuuuuuge would become huuuge\n",
        "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "\n",
        "#lemmatizer gets stems of words (eg. dogs becomes dog, saddens becomes sad)\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def make_features(tweet):\n",
        "\n",
        "    #some more preprocessing\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(r\"'s?\", \"\", tweet)\n",
        "\n",
        "    words = tokenizer.tokenize(tweet)\n",
        "    words = [lemmatizer.lemmatize(t) for t in words]\n",
        "\n",
        "    return {word:True for word in words if not is_stop_word(word)}\n",
        "\n",
        "#good split to have low variance in test accuracy\n",
        "train_frac = 80/100\n",
        "\n",
        "processed = [(make_features(tweets[i]), labels[i]) for i in range(len(labels))]\n",
        "\n",
        "train = processed[:int(len(processed)*train_frac)]\n",
        "\n",
        "test = processed[int(len(processed)*train_frac):]\n",
        "testpos = [t for t in test if t[1] == 4]\n",
        "testneg = [t for t in test if t[1] == 0]\n",
        "\n",
        "print(f\"Training sample size: {len(train)}\")\n",
        "print(f\"Testing sample size: {len(test)}\")\n",
        "print(f\"Split of pos/neg labels in test data: {len(testpos)} : {len(testneg)}\")\n",
        "\n",
        "classifier = NaiveBayesClassifier.train(train)\n",
        "\n",
        "print(\"\\nOverall train accuracy:\", util.accuracy(classifier, train))\n",
        "print(\"Overall test accuracy:\", util.accuracy(classifier, test))\n",
        "\n",
        "#try on different test sets to check for bias\n",
        "print(\"\\nTest accuracy on pos:\", util.accuracy(classifier, testpos))\n",
        "print(\"Test accuracy on neg:\", util.accuracy(classifier, testneg))\n",
        "\n",
        "#prints the words that were the strongest indicators of sentiment\n",
        "classifier.show_most_informative_features()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training sample size: 1280000\n",
            "Testing sample size: 320000\n",
            "Split of pos/neg labels in test data: 159743 : 160257\n",
            "\n",
            "Overall train accuracy: 0.8041265625\n",
            "Overall test accuracy: 0.77084375\n",
            "\n",
            "Test accuracy on pos: 0.715192527998097\n",
            "Test accuracy on neg: 0.8263164791553567\n",
            "Most Informative Features\n",
            "                 saddens = True                0 : 4      =     57.7 : 1.0\n",
            "                saddened = True                0 : 4      =     54.6 : 1.0\n",
            "                   saddd = True                0 : 4      =     51.0 : 1.0\n",
            "                dividend = True                4 : 0      =     43.0 : 1.0\n",
            "             shareholder = True                4 : 0      =     41.0 : 1.0\n",
            "                  farrah = True                0 : 4      =     39.2 : 1.0\n",
            "                  ouchhh = True                0 : 4      =     37.0 : 1.0\n",
            "                     447 = True                0 : 4      =     35.6 : 1.0\n",
            "                  ouchie = True                0 : 4      =     33.8 : 1.0\n",
            "                    sadd = True                0 : 4      =     32.6 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10nqt80pIub6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#try it out\n",
        "\n",
        "text = \"vote for trump\" \n",
        "#TODO: mixture model is probably gonna be the only thing that classifies this\n",
        "# properly. at least i hope\n",
        "\n",
        "#TODO: filtering out subject of sentiment \n",
        "# analysis since we want sentiment about the subject and don't want the subject's own associated sentiment to bias our results\n",
        "print(classifier.classify(features(text)))\n",
        "print(TextBlob(text).sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsN6kZAD6-Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compare to famous TextBlob's accuracy, we actually do much better\n",
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "\n",
        "correct_count = 0\n",
        "n = len(labels)//5\n",
        "for i in range(n):\n",
        "  #Note: adding our preprocessing makes it perform worse so I skip that for\n",
        "  # a fair comparison\n",
        "  if (TextBlob(tweet).sentiment.polarity > 0):\n",
        "    if labels[i] == 4:\n",
        "      correct_count += 1\n",
        "  else:\n",
        "    if labels[i] == 0:\n",
        "      correct_count += 1\n",
        "\n",
        "acc = (correct_count/n)*100\n",
        "print(acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}